{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the inverted index part of this project.\n",
    "\n",
    "Some current thoughts\n",
    "1. read in the json file\n",
    "2. pre-process the ingredients\n",
    "   1. this might be a sample one for now, we'll need Jess's clean data. \n",
    "3. build the inverted index for ingredient\n",
    "4. save the inverted index\n",
    "\n",
    "\n",
    "some of the data requirements:\n",
    "1. no numbers\n",
    "2. no stopwords\n",
    "3. clean \"Advertisement\"\n",
    "4. no word such as \"with, next, and etc\"--are these involved in stop words?\n",
    "5. no units (maybe) such as cups, pounds, and etc\n",
    "6. no punctuations\n",
    "7. tokenization\n",
    "8. stemm\n",
    "9. no capital letters (if not necessary)\n",
    "10. save it as a clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.DFpostings = {}\n",
    "\n",
    "    def preProcess(self, text):\n",
    "        #define stemmer and stopwords\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        #clean and tokenize\n",
    "        text = text.lower()\n",
    "        text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
    "\n",
    "        return filtered_tokens\n",
    "\n",
    "    def indexFile(self, ingredients, fileId):\n",
    "        processed_ingredients = self.preProcess(' '.join(ingredients))\n",
    "        for token in set(processed_ingredients):\n",
    "            if token in self.DFpostings:\n",
    "                self.DFpostings[token].append(fileId)\n",
    "            else:\n",
    "                self.DFpostings[token] = [fileId]\n",
    "\n",
    "    def save(self):\n",
    "        with open(\"DFPostings.json\", \"w\") as file:\n",
    "            json.dump(self.DFpostings, file)\n",
    "\n",
    "#load json data\n",
    "with open('./sample_data.json', 'r') as file:\n",
    "    recipes = json.load(file)\n",
    "\n",
    "#initialize inverted index\n",
    "index = InvertedIndex()\n",
    "\n",
    "#index each recipe's ingredients\n",
    "for recipe_id, recipe_info in recipes.items():\n",
    "    index.indexFile(recipe_info['ingredients'], recipe_id)\n",
    "\n",
    "#save the index to a file\n",
    "index.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InvertedIndex' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m index\u001b[38;5;241m.\u001b[39mindexRecipes(recipes)\n\u001b[0;32m     50\u001b[0m index\u001b[38;5;241m.\u001b[39msaveIndex()\n\u001b[1;32m---> 52\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# user query input\u001b[39;00m\n\u001b[0;32m     55\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter ingredients separated by commas: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'InvertedIndex' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "###this is only a try\n",
    "\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.DFpostings = {}\n",
    "        self.recipes = {}  #store full recipe details for retrieval\n",
    "\n",
    "    def preProcess(self, text):\n",
    "        #stem\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        #stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        #lower letter\n",
    "        text = text.lower()\n",
    "        text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "        #tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
    "        return filtered_tokens\n",
    "\n",
    "    def indexRecipes(self, recipes):\n",
    "        for recipe_id, recipe_info in recipes.items():\n",
    "            processed_ingredients = self.preProcess(' '.join(recipe_info['ingredients']))\n",
    "            self.recipes[recipe_id] = {'ingredients': recipe_info['ingredients'], 'instructions': recipe_info['instructions']}\n",
    "            for token in set(processed_ingredients):\n",
    "                if token in self.DFpostings:\n",
    "                    self.DFpostings[token].append(recipe_id)\n",
    "                else:\n",
    "                    self.DFpostings[token] = [recipe_id]\n",
    "\n",
    "    def saveIndex(self):\n",
    "        with open(\"DFPostings.json\", \"w\") as file:\n",
    "            json.dump(self.DFpostings, file)\n",
    "        with open(\"Recipes.json\", \"w\") as file:\n",
    "            json.dump(self.recipes, file)\n",
    "\n",
    "    def search(self, query):\n",
    "        query_tokens = self.preProcess(query)\n",
    "        matched_recipes = set()\n",
    "        for token in query_tokens:\n",
    "            if token in self.DFpostings:\n",
    "                matched_recipes.update(self.DFpostings[token])\n",
    "        return [(self.recipes[recipe_id]['ingredients'], self.recipes[recipe_id]['instructions']) for recipe_id in matched_recipes]\n",
    "\n",
    "#load and index recipes\n",
    "with open('./sample_data.json', 'r') as file:\n",
    "    recipes = json.load(file)\n",
    "index = InvertedIndex()\n",
    "index.indexRecipes(recipes)\n",
    "index.saveIndex()\n",
    "\n",
    "# user query input\n",
    "user_input = input(\"Enter ingredients separated by commas: \")\n",
    "results = index.search(user_input)\n",
    "\n",
    "#display results\n",
    "for ingredients, instructions in results:\n",
    "    print(\"Ingredients:\", ingredients)\n",
    "    print(\"Instructions:\", instructions)\n",
    "    print(\"\\n---\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5200",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
